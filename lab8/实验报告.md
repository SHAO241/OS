# 练习1

## sfs_io_nolock函数

### 核心算法流程
```txt
输入：文件F，偏移O，长度L
1. 计算逻辑块号：blkno = O / BLOCK_SIZE
2. 计算块内偏移：blkoff = O % BLOCK_SIZE
3. 计算结束位置：endpos = O + L（不超过文件大小）
4. 计算完整块数：nblks

5. 处理：
   - 第一部分：如果blkoff≠0，读写第一个部分块
   - 第二部分：循环处理所有完整块（nblks个）
   - 第三部分：如果结束位置不在边界，读写最后一个部分块

6. 更新：
   - 实际读写长度（alenp）
   - 文件大小（如果是写入且扩展了文件）
```

### 主要逻辑
函数将读写请求智能地分解为三部分处理：
```txt
[起始不对齐部分][完整块][末尾不对齐部分]
     ↓               ↓          ↓
 部分块处理 ← 整块处理 → 部分块处理
```
#### 1. 处理起始部分的不对齐 (Handle unaligned start)
```cpp    
    // 计算在当前块内的偏移量
    if ((blkoff = offset % SFS_BLKSIZE) != 0) {
        // 如果 nblks != 0，说明跨越了块，读取该块剩余部分；否则读取直到 endpos
        size = (nblks != 0) ? (SFS_BLKSIZE - blkoff) : (endpos - offset);
        
        // 获取对应的磁盘块号 (ino)
        if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
            goto out;
        }
        // 执行读写操作
        if ((ret = sfs_buf_op(sfs, buf, size, ino, blkoff)) != 0) {
            goto out;
        }
        
        // 更新指针和计数器
        alen += size;
        buf += size;
        
        // 如果起始和结束在同一个块内，现在已经完成了，退出
        if (nblks == 0) {
            goto out;
        }
        
        // 我们已经处理了一个块边界，因此减少剩余完整块数，并指向下一个逻辑块
        blkno++;
        nblks--;
    }
```

#### 2. 处理中间的完整块 (Handle aligned full blocks)
``` cpp  
    if (nblks > 0) {
        int i;
        for (i = 0; i < nblks; i++) {
            // 获取块号
            if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
                goto out;
            }
            // 整块读写 (sfs_block_op 通常比 buf_op 快)
            if ((ret = sfs_block_op(sfs, buf, ino, 1)) != 0) {
                goto out;
            }
            
            alen += SFS_BLKSIZE;
            buf += SFS_BLKSIZE;
            blkno++;
        }
    }
```

#### 3. 处理末尾的不对齐 (Handle unaligned end)
``` cpp   
    // 如果结束位置不在块边界上，说明最后一块只有前半部分数据
    if ((size = endpos % SFS_BLKSIZE) != 0) {
        if ((ret = sfs_bmap_load_nolock(sfs, sin, blkno, &ino)) != 0) {
            goto out;
        }
        // 从该块的偏移 0 开始读写 size 长度
        if ((ret = sfs_buf_op(sfs, buf, size, ino, 0)) != 0) {
            goto out;
        }
        alen += size;
    }
```
### 关键点

#### 逻辑块到物理块的映射
使用 sfs_bmap_load_nolock 将逻辑块号转换为物理块号，这是文件系统处理磁盘布局的核心
``` cpp
static int
sfs_bmap_load_nolock(struct sfs_fs *sfs, struct sfs_inode *sin, uint32_t index, uint32_t *ino_store) {
    struct sfs_disk_inode *din = sin->din;
    assert(index <= din->blocks);    // 确保逻辑块号不超过当前文件块数
    int ret;
    uint32_t ino;
    bool create = (index == din->blocks);
    if ((ret = sfs_bmap_get_nolock(sfs, sin, index, create, &ino)) != 0) {
        return ret;
    }
    assert(sfs_block_inuse(sfs, ino));    // 确保返回的磁盘块确实被使用
    if (create) {
        din->blocks ++;    // 如果是创建新块，增加文件块计数
    }
    if (ino_store != NULL) {
        *ino_store = ino;    // 将物理块号传递给调用者
    }
    return 0;
}
```
**核心作用：** 给定一个文件的逻辑块号（index），找到对应的物理磁盘块号（ino）

**输入参数：**
- sfs:       文件系统结构体（包含整个文件系统的信息）
- sin:       文件的inode结构体（包含该文件的具体信息）
- index:     逻辑块号（从0开始，相对于文件开头）

**输出参数：**
- ino_store: 存储返回的物理磁盘块号

**核心映射调用：**
```cpp
if ((ret = sfs_bmap_get_nolock(sfs, sin, index, create, &ino)) != 0) {
    return ret;
}
```
调用 sfs_bmap_get_nolock 执行实际的块映射查找：
create = true：如果需要，分配新磁盘块
create = false：只查找现有映射
ino：返回的物理磁盘块号


#### 性能优化
部分块：使用 sfs_buf_op（缓冲操作）
完整块：使用 sfs_block_op（块操作，更快）
**减少了磁盘访问次数**


# 练习2

## load_icode函数主要修改点
**从内存加载 ELF 程序到从文件加载 ELF 程序：**

### 1. **函数参数和调用方式改变**
```c
// 修改前：从内存缓冲区加载
static int load_icode(unsigned char *binary, size_t size)

// 修改后：从文件描述符加载
static int load_icode(int fd, int argc, char **kargv)
```
**作用**：
- 从**内存中的二进制数据**改为**从磁盘文件读取**
- 支持**命令行参数传递**（argc, kargv）
- 更符合实际操作系统执行程序的方式

### 2. **新增文件读取辅助函数**
```c
static int load_icode_read(int fd, void *buf, size_t len, off_t offset)
```
**作用**：
- 封装文件读取操作
- 支持随机访问（通过 offset 参数）
- 简化主函数中的文件读取逻辑

### 3. **ELF 头部加载方式改变**
```c
// 修改前：直接从内存指针读取
struct elfhdr *elf = (struct elfhdr *)binary;

// 修改后：从文件读取到本地变量
struct elfhdr elf;
ret = load_icode_read(fd, &elf, sizeof(struct elfhdr), 0);
```
**作用**：
- 不需要一次性将整个文件读入内存
- 可以处理大型可执行文件
- 更节省内存

### 4. **程序段加载方式优化**
```c
// 修改前：直接从内存复制
memcpy(page2kva(page) + off, from, size);

// 修改后：从文件分段读取
if (load_icode_read(fd, kva + off, read_len, offset) != 0)
```
**作用**：
- 避免将整个程序文件一次性加载到内核内存
- 支持按需加载（虽然这里还是全量加载）
- offset 精确控制文件读取位置

### 5. **BSS 段处理的改进**
```c
// 修改前：复杂的两个循环分别处理 TEXT/DATA 和 BSS
// 修改后：统一在一个循环中处理
if (start < ph_base->p_va + ph_base->p_filesz) {
    // 从文件读取数据
    load_icode_read(fd, kva + off, read_len, offset);
    // 剩余部分清零（BSS部分）
    memset(kva + off + read_len, 0, size - read_len);
} else {
    // 纯 BSS 页面，全部清零
    memset(kva + off, 0, size);
}
```
**作用**：
- 逻辑更清晰，减少了代码复杂度
- 正确处理 filesz < memsz 的情况
- 确保 BSS 段被正确初始化为0

### 6. **参数传递机制的重构**
```c
// 新增：参数处理代码
char **uargv = (char **)kmalloc(sizeof(char *) * argc);
// 将参数字符串复制到用户栈
// 设置 trapframe 中的参数寄存器
tf->gpr.a0 = argc;          // RISC-V 中 a0 存放 argc
tf->gpr.a1 = uargv_base;    // a1 存放 argv 数组地址
```
**作用**：
- 支持命令行参数传递
- 符合 Unix/Linux 程序调用规范
- 通过用户栈传递参数

### 7. **用户栈初始化的改进**
```c
// 修改前：简单的页分配
assert(pgdir_alloc_page(mm->pgdir, USTACKTOP - PGSIZE, PTE_USER) != NULL);

// 修改后：分配栈空间并处理参数
// 1. 分配栈页面
// 2. 将参数字符串压栈
// 3. 设置栈顶指针
```
**作用**：
- 创建完整的用户栈环境
- 包含命令行参数
- 符合实际程序执行环境

### 核心改进总结

#### **改进1：从内存加载到文件加载**
```
之前：binary → 内存 → 直接复制
现在：文件 → 按需读取 → 加载到进程空间
```
**优点**：更符合实际，支持大文件，减少内存占用

#### **改进2：支持命令行参数**
```
之前：无参数支持
现在：完整的 argc/argv 机制
```
**优点**：使程序更完整，符合 Unix 标准

#### **改进3：BSS 段处理优化**
```
之前：两个独立循环，逻辑复杂
现在：统一处理，逻辑清晰
```
**优点**：减少代码重复，避免潜在错误

#### **改进4：RISC-V 架构适配**
```c
// 修改权限位设置
if (vm_flags & VM_READ) perm |= PTE_R;
if (vm_flags & VM_WRITE) perm |= PTE_W;  // 修改前是 PTE_W | PTE_R
if (vm_flags & VM_EXEC) perm |= PTE_X;
```
**优点**：正确设置 RISC-V 页表权限

修改代码使得 `load_icode` 函数从一个简单的内存加载器变成了一个完整的**程序加载器**，更接近真实操作系统的 `execve` 系统调用实现。

## 其它主要修改点分析

### 1. **文件系统支持的头文件引入**
```c
// 新增的头文件
#include <fs.h>
#include <vfs.h>
#include <sysfile.h>
```
**作用**：支持文件系统操作，为从内存加载到文件加载的转变做准备

### 2. **进程结构体字段初始化（alloc_proc）**
```c
// 修改前：只有基本字段初始化
// 修改后：增加了LAB8的字段初始化
proc->rq = NULL;              // 运行队列指针
list_init(&(proc->run_link)); // 运行队列链表
proc->time_slice = 0;        // 时间片
// 还有 stride 调度相关字段...
```
**作用**：
- 初始化调度相关数据结构
- 避免未初始化导致的错误

### 3. **进程运行函数（proc_run）的改进**
```c
// 新增了TLB刷新
lsatp(next->pgdir);
flush_tlb();  // [重要]刷新TLB，确保地址映射正确
```
**作用**：
- 在RISC-V架构下，切换页表后必须刷新TLB
- 避免旧的TLB条目导致内存访问错误
- 保证进程地址空间的隔离性

### 4. **新增文件系统操作函数**

#### **copy_files 函数**
```c
static int copy_files(uint32_t clone_flags, struct proc_struct *proc)
```
**作用**：
- 复制或共享父进程的文件描述符表
- 支持 `CLONE_FS` 标志（共享文件系统上下文）
- 实现了 `fork()` 时文件描述符的继承

#### **put_files 函数**
```c
static void put_files(struct proc_struct *proc)
```
**作用**：
- 减少文件描述符表的引用计数
- 当引用计数为0时销毁文件描述符表
- 避免文件资源泄漏

### 5. **fork函数（do_fork）的重大重构**

#### **添加文件系统复制**
```c
// 在 copy_mm 之后调用 copy_files
if (copy_files(clone_flags, proc) != 0) {
    goto bad_fork_cleanup_mm;
}
```
**作用**：
- 在创建子进程时复制文件描述符
- 支持父子进程共享或复制文件资源

#### **错误处理流程修正**
```c
// 新增的错误处理标签
bad_fork_cleanup_fs:    // 文件系统清理
bad_fork_cleanup_mm:    // 内存管理清理
```
**作用**：
- 更精细的资源清理顺序
- 避免资源泄漏
- 确保所有分配的资源都能正确释放

### 6. **退出函数（do_exit）的完善**
```c
// 新增文件系统资源释放
put_files(current);  // 释放当前进程的文件资源
```
**作用**：
- 进程退出时释放文件描述符
- 减少文件描述符表的引用计数

### 7. **execve函数（do_execve）的完全重写**

#### **从内存参数改为文件参数**
```c
// 修改前：
int do_execve(const char *name, size_t len, unsigned char *binary, size_t size)

// 修改后：
int do_execve(const char *name, int argc, const char **argv)
```
**作用**：
- 支持命令行参数传递
- 从文件名而非内存二进制数据加载程序
- 更接近实际操作系统接口

#### **新增辅助函数**
```c
static void put_kargv(int argc, char **kargv)  // 释放内核参数
static int copy_kargv(...)                     // 从用户空间复制参数
static bool copy_string(...)                   // 复制字符串（未在代码中但被调用）
```
**作用**：
- 安全的参数传递机制
- 用户空间到内核空间的参数复制
- 内存管理和错误处理

#### **文件系统操作整合**
```c
// 打开可执行文件
if ((ret = fd = sysfile_open(path, O_RDONLY)) < 0) {
    goto execve_exit;
}
// 关闭所有文件描述符
files_closeall(current->filesp);
```
**作用**：
- 从文件系统加载可执行文件
- 执行新程序前清理旧环境

### 8. **用户栈参数处理的改进**
```c
// 修改前：直接使用用户虚拟地址
memcpy((void *)stacktop, kargv[i], len);

// 修改后：通过KVA安全复制
pte_t *pte = get_pte(mm->pgdir, stacktop, 0);
uintptr_t kva = (uintptr_t)page2kva(page) + (stacktop & (PGSIZE - 1));
memcpy((void *)kva, kargv[i], len);
```
**作用**：
- 正确处理进程切换后的地址空间
- 避免在用户地址空间直接操作
- 确保参数正确复制到用户栈

### 9. **初始化函数（proc_init）的修改**
```c
// 为idle进程初始化文件系统
if ((idleproc->filesp = files_create()) == NULL) {
    panic("create filesp (idleproc) failed.\n");
}
files_count_inc(idleproc->filesp);
```
**作用**：
- 系统启动时为idle进程创建文件描述符表
- 保证所有进程都有有效的文件系统上下文

### 10. **内核执行（kernel_execve）接口变化**
```c
// 修改前：直接传递二进制数据
static int kernel_execve(const char *name, unsigned char *binary, size_t size)

// 修改后：传递参数数组
static int kernel_execve(const char *name, const char **argv)
```
**作用**：
- 支持命令行参数
- 与新的 `do_execve` 接口保持一致

### 11. **新增进程睡眠函数（do_sleep）**
```c
int do_sleep(unsigned int time)
```
**作用**：
- 支持定时睡眠系统调用
- 使用定时器实现进程暂停
- 超时后自动唤醒进程

这些修改使得ucore从一个简单的教学操作系统，向一个更完整、更接近真实Linux的操作系统迈进，特别是在进程管理、文件系统和内存安全方面有了显著提升。

## make qemu之后显示：
``` txt

OpenSBI v0.4 (Jul  2 2019 11:53:53)
   ____                    _____ ____ _____
  / __ \                  / ____|  _ \_   _|
 | |  | |_ __   ___ _ __ | (___ | |_) || |
 | |  | | '_ \ / _ \ '_ \ \___ \|  _ < | |
 | |__| | |_) |  __/ | | |____) | |_) || |_
  \____/| .__/ \___|_| |_|_____/|____/_____|
        | |
        |_|

Platform Name          : QEMU Virt Machine
Platform HART Features : RV64ACDFIMSU
Platform Max HARTs     : 8
Current Hart           : 0
Firmware Base          : 0x80000000
Firmware Size          : 112 KB
Runtime SBI Version    : 0.1

PMP0: 0x0000000080000000-0x000000008001ffff (A)
PMP1: 0x0000000000000000-0xffffffffffffffff (A,R,W,X)
(THU.CST) os is loading ...

Special kernel symbols:
  entry  0xc020004a (virtual)
  etext  0xc020b8aa (virtual)
  edata  0xc0291060 (virtual)
  end    0xc0296918 (virtual)
Kernel executable memory footprint: 603KB
DTB Init
HartID: 0
DTB Address: 0x82200000
Physical Memory from DTB:
  Base: 0x0000000080000000
  Size: 0x0000000008000000 (128 MB)
  End:  0x0000000087ffffff
DTB init completed
memory management: default_pmm_manager
physcial memory map:
  memory: 0x08000000, [0x80000000, 0x87ffffff].
vapaofset is 18446744070488326144
check_alloc_page() succeeded!
Page table directory switch succeeded!
Kernel stack guardians set succeeded!
check_pgdir() succeeded!
check_boot_pgdir() succeeded!
use SLOB allocator
kmalloc_init() succeeded!
check_vma_struct() succeeded!
check_vmm() succeeded.
sched class: RR_scheduler
Initrd: 0xc0214010 - 0xc021bd0f, size: 0x00007d00
Initrd: 0xc021bd10 - 0xc029100f, size: 0x00075300
sfs: mount: 'simple file system' (106/11/117)
vfs: mount disk0.
++ setup timer interrupts
kernel_execve: pid = 2, name = "sh".
user sh is running!!!
Hello world!!.
I am process 3.
hello pass.
I am the parent. Forking the child...
I am parent, fork a child pid 5
I am the parent, waiting now..
I am the child.
waitpid 5 ok.
exit pass.
$ 
```

# 扩展练习 Challenge1：完成基于"UNIX的PIPE机制"的设计方案

## 1. PIPE机制概述

UNIX管道是一种进程间通信(IPC)机制，允许一个进程的输出作为另一个进程的输入。管道分为匿名管道(pipe)和命名管道(FIFO)两种类型。

## 2. 数据结构设计

### 2.1 管道缓冲区结构
```c
#define PIPE_BUF_SIZE 4096  // 管道缓冲区大小

struct pipe_buffer {
    char data[PIPE_BUF_SIZE];    // 环形缓冲区
    int read_pos;                // 读指针位置
    int write_pos;               // 写指针位置
    int count;                   // 当前数据量
    semaphore_t mutex;           // 互斥访问缓冲区
    semaphore_t not_full;        // 缓冲区非满信号量
    semaphore_t not_empty;       // 缓冲区非空信号量
    int readers;                 // 读端引用计数
    int writers;                 // 写端引用计数
    bool closed_read;            // 读端是否关闭
    bool closed_write;           // 写端是否关闭
};
```

### 2.2 管道文件描述符结构
```c
struct pipe_inode {
    struct pipe_buffer *buffer;  // 指向管道缓冲区
    bool is_read_end;           // 是否为读端
    bool is_write_end;          // 是否为写端
};
```

### 2.3 扩展file结构
在现有的`struct file`基础上，需要添加管道类型支持：
```c
struct file {
    enum {
        FD_NONE, FD_INIT, FD_OPENED, FD_CLOSED,
        FD_PIPE_READ, FD_PIPE_WRITE  // 新增管道类型
    } status;
    bool readable;
    bool writable;
    int fd;
    off_t pos;
    union {
        struct inode *node;          // 普通文件
        struct pipe_inode *pipe;     // 管道文件
    };
    int open_count;
};
```

## 3. 接口设计

### 3.1 系统调用接口
```c
// 创建匿名管道，返回读写文件描述符
int sys_pipe(int pipefd[2]);

// 创建命名管道(FIFO)
int sys_mkfifo(const char *pathname, mode_t mode);
```

### 3.2 内核接口
```c
// 管道缓冲区管理
struct pipe_buffer *pipe_buffer_create(void);
void pipe_buffer_destroy(struct pipe_buffer *buffer);

// 管道读写操作
int pipe_read(struct pipe_buffer *buffer, void *buf, size_t count);
int pipe_write(struct pipe_buffer *buffer, const void *buf, size_t count);

// 管道文件操作
int pipe_open(struct pipe_inode *pipe, uint32_t flags);
int pipe_close(struct pipe_inode *pipe);
```

## 4. 同步互斥处理

### 4.1 缓冲区访问同步
- 使用`mutex`信号量保证对缓冲区的互斥访问
- 使用`not_full`信号量控制写操作，当缓冲区满时阻塞写进程
- 使用`not_empty`信号量控制读操作，当缓冲区空时阻塞读进程

### 4.2 管道生命周期管理
- 通过引用计数`readers`和`writers`管理管道的生命周期
- 当所有写端关闭时，读操作返回EOF
- 当所有读端关闭时，写操作产生SIGPIPE信号

### 4.3 死锁避免
- 读写操作使用非阻塞方式获取信号量，避免死锁
- 实现超时机制，防止进程无限期等待

## 5. 实现要点

### 5.1 环形缓冲区实现
```c
int pipe_write_internal(struct pipe_buffer *buffer, const void *buf, size_t count) {
    // [P操作/加锁] 获取互斥锁，进入临界区，保护 buffer 结构体不被并发修改
    down(&buffer->mutex);
    
    // 循环检查：当缓冲区已满 且 读端未关闭时
    // 这里使用 while 而不是 if，是为了防止虚假唤醒（spurious wakeup）或被唤醒后空间又被抢占
    while (buffer->count == PIPE_BUF_SIZE && !buffer->closed_read) {
        // [V操作/解锁] 暂时释放互斥锁
        // 必须释放锁，否则读进程无法获取锁来读取数据，会导致死锁
        up(&buffer->mutex);
        
        // [P操作/等待] 等待“非满”信号量 (not_full)
        // 进程在此处阻塞（睡眠），直到读进程取走数据并发出 not_full 信号
        down(&buffer->not_full);  
        
        // 醒来后，[P操作/加锁] 重新获取互斥锁，准备再次检查 while 条件
        down(&buffer->mutex);
    }
    
    // 边界情况处理：检查读端是否已经关闭
    // 如果读端关了，再往里写数据就没有意义了（Broken Pipe）
    if (buffer->closed_read) {
        up(&buffer->mutex); // 返回前记得释放锁
        return -EPIPE;      // 返回错误码 -EPIPE (通常对应 errno 的 Broken pipe)
    }
    
    // 开始写入数据到环形缓冲区
    int written = 0;
    
    // 循环写入：只要 1. 还有数据没写完 (written < count)
    //          且  2. 缓冲区还没满 (buffer->count < PIPE_BUF_SIZE)
    while (written < count && buffer->count < PIPE_BUF_SIZE) {
        // 将用户数据的一个字节拷贝到缓冲区当前写指针位置
        buffer->data[buffer->write_pos] = ((char*)buf)[written];
        
        // 更新写指针：使用取模运算 (%) 实现环形逻辑
        // 如果到了数组末尾，write_pos 会自动回到 0
        buffer->write_pos = (buffer->write_pos + 1) % PIPE_BUF_SIZE;
        
        // 更新缓冲区内的有效数据计数
        buffer->count++;
        
        // 更新已写入字节计数
        written++;
    }
    
    // [V操作/解锁] 写入完成，离开临界区
    up(&buffer->mutex);
    
    // [V操作/唤醒] 发送“非空”信号 (not_empty)
    // 唤醒可能因为缓冲区空而正在休眠的读进程
    up(&buffer->not_empty);  
    
    // 返回实际写入的字节数
    return written;
}
```

### 5.2 进程fork时的管道处理
- 子进程继承父进程的文件描述符表
- 管道的引用计数需要相应增加
- 实现写时复制(COW)机制优化性能


# 扩展练习 Challenge2：完成基于"UNIX的软连接和硬连接机制"的设计方案

## 1. 链接机制概述

UNIX文件系统支持两种链接类型：
- **硬链接(Hard Link)**：多个文件名指向同一个inode，共享相同的数据块
- **软链接(Symbolic Link)**：包含指向另一个文件路径的特殊文件

## 2. 数据结构设计

### 2.1 扩展inode结构
```c
struct sfs_inode {
    struct sfs_disk_inode *din;     // 磁盘inode
    uint32_t ino;                   // inode编号
    bool dirty;                     // 是否需要写回磁盘
    int reclaim_count;              // 回收计数
    semaphore_t sem;                // inode信号量
    list_entry_t inode_link;        // inode链表
    list_entry_t hash_link;         // hash链表
    
    // 新增字段
    uint32_t nlinks;                // 硬链接计数
    uint32_t type;                  // 文件类型(普通文件/目录/符号链接)
};
```

### 2.2 磁盘inode结构扩展
```c
struct sfs_disk_inode {
    uint32_t size;                  // 文件大小
    uint16_t type;                  // 文件类型
    uint16_t nlinks;                // 硬链接计数
    uint32_t blocks;                // 数据块数量
    uint32_t direct[SFS_NDIRECT];   // 直接数据块索引
    uint32_t indirect;              // 间接数据块索引
    
    // 新增字段用于符号链接
    char symlink_target[SFS_MAX_SYMLINK_LEN];  // 符号链接目标路径
};
```

### 2.3 目录项结构
```c
struct sfs_disk_entry {
    uint32_t ino;                   // inode编号
    char name[SFS_MAX_FNAME_LEN + 1]; // 文件名
    uint8_t file_type;              // 文件类型标识
};
```

### 2.4 文件类型定义
```c
#define SFS_TYPE_INVAL    0         // 无效类型
#define SFS_TYPE_FILE     1         // 普通文件
#define SFS_TYPE_DIR      2         // 目录
#define SFS_TYPE_LINK     3         // 符号链接
#define SFS_MAX_SYMLINK_LEN 256     // 符号链接最大长度
```

## 3. 接口设计

### 3.1 系统调用接口
```c
// 创建硬链接
int sys_link(const char *oldpath, const char *newpath);

// 创建符号链接
int sys_symlink(const char *target, const char *linkpath);

// 读取符号链接内容
ssize_t sys_readlink(const char *pathname, char *buf, size_t bufsiz);

// 删除链接
int sys_unlink(const char *pathname);
```

### 3.2 VFS层接口
```c
// inode操作扩展
int vop_link(struct inode *old_node, struct inode *dir_node, const char *name);
int vop_symlink(struct inode *dir_node, const char *name, const char *target);
int vop_readlink(struct inode *node, struct iobuf *iob);
```

### 3.3 SFS文件系统接口
```c
// 硬链接操作
int sfs_link(struct inode *old_node, struct inode *dir_node, const char *name);
int sfs_unlink(struct inode *dir_node, const char *name);

// 符号链接操作
int sfs_symlink(struct inode *dir_node, const char *name, const char *target);
int sfs_readlink(struct inode *node, char *buf, size_t len);

// 链接计数管理
int sfs_nlinks_inc(struct inode *node);
int sfs_nlinks_dec(struct inode *node);
```

## 4. 同步互斥处理

### 4.1 inode级别同步
```c
// inode操作需要获取inode信号量以保证原子性
int sfs_link_safe(struct inode *old_node, struct inode *dir_node, const char *name) {
    // [P操作/加锁] 首先获取“源文件”inode的信号量
    // 保护 old_node 的元数据（如引用计数 nlinks）不被其他进程并发修改
    down(&old_node->sem);
    
    // [P操作/加锁] 然后获取“目标目录”inode的信号量
    // 保护 dir_node 的内容（目录项列表），防止两个进程同时往同一个目录写文件导致数据错乱
    down(&dir_node->sem);
    
    // 调用内部非线程安全版本执行实际操作
    // 此时已经持有了两把锁，可以安全地修改 inode 数据和目录项
    // 核心逻辑通常包括：
    // 1. 在 dir_node 目录下创建一个新条目 'name' 指向 old_node
    // 2. 增加 old_node->nlinks 计数
    int ret = sfs_link_internal(old_node, dir_node, name);
    
    // [V操作/解锁] 操作完成，释放目标目录的锁
    // 注意：解锁顺序通常与加锁顺序相反（LIFO），虽然对互斥锁这不是强制的，但符合栈的逻辑
    up(&dir_node->sem);
    
    // [V操作/解锁] 释放源文件的锁
    up(&old_node->sem);
    
    // 返回操作结果（0表示成功，负数表示错误码）
    return ret;
}
```

### 4.2 目录操作同步
- 目录修改操作需要获取目录的写锁
- 多个进程同时创建/删除链接时需要序列化
- 使用读写锁提高并发性能

### 4.3 引用计数同步
```c
// 原子地增加 inode 的硬链接计数 (nlinks)
static inline void sfs_nlinks_inc_atomic(struct sfs_inode *sin) {
    // [原子操作] 引用计数 +1
    // 使用原子指令是为了防止多进程同时读取同一个文件时导致计数竞争出错
    atomic_inc(&sin->nlinks);
    
    // 标记 inode 为“脏” (dirty)
    // 表示内存中的 inode 数据已被修改，与磁盘上的数据不一致
    // 文件系统同步(sync)时会根据此标志将数据回写到磁盘
    sin->dirty = true;
}

// 原子地减少 inode 的硬链接计数，并返回减少后的值
static inline int sfs_nlinks_dec_atomic(struct sfs_inode *sin) {
    // [原子操作] 引用计数 -1，并立即获取减少后的结果
    // 必须使用 atomic_dec_return 而不是 atomic_dec，
    // 因为我们需要知道减完之后是否变成了 0
    int nlinks = atomic_dec_return(&sin->nlinks);
    
    // 同样标记为“脏”，等待回写磁盘
    sin->dirty = true;
    
    // 返回当前的引用计数
    // 调用者通常会检查返回值：如果为 0，说明没有任何目录指向该文件了，
    // 此时文件系统应该回收该 inode 及其占用的数据块
    return nlinks;
}
```

## 5. 实现要点

### 5.1 硬链接实现
```c
// 创建硬链接：让 dir_node 下的 name 指向 old_node 对应的文件
int sfs_link(struct inode *old_node, struct inode *dir_node, const char *name) {
    // 1. 检查文件类型：防止文件系统出现环路
    // 大多数文件系统（包括 Linux）禁止对目录创建硬链接
    // 如果允许目录硬链接，目录树结构可能变成有向有环图，导致遍历（如 find 命令）死循环
    if (sfs_get_type(old_node) == SFS_TYPE_DIR) {
        return -EPERM; // Operation not permitted
    }
    
    // 2. 准备新的目录项 (Disk Entry)
    struct sfs_disk_entry entry;
    
    // 关键点：将新条目的 inode 编号设置为源文件的 inode 编号
    // 这就是硬链接的定义：两个文件名共享同一个 inode
    entry.ino = sfs_get_ino(old_node);
    
    // 复制文件名，注意长度限制
    strncpy(entry.name, name, SFS_MAX_FNAME_LEN);
    
    // 设置条目类型
    entry.file_type = SFS_TYPE_FILE;
    
    // 3. 将新目录项写入到目标目录的数据块中
    // sfs_add_direntry 内部通常会检查是否有重名文件
    int ret = sfs_add_direntry(dir_node, &entry);
    
    // 4. 如果目录项添加成功，更新源文件的元数据
    if (ret == 0) {
        // 原子地增加源文件的硬链接计数 (nlinks)
        // 只要 nlinks > 0，文件的数据就不会被释放
        // vop_info 将抽象的 VFS inode 转换为具体的 SFS inode
        sfs_nlinks_inc_atomic(vop_info(old_node, sfs_inode));
    }
    
    return ret;
}
```

### 5.2 符号链接实现
```c
int sfs_symlink(struct inode *dir_node, const char *name, const char *target) {
    // 1. 创建一个新的 inode
    // 区别于硬链接（复用旧 inode），软链接必须分配一个新的 Inode 节点
    struct inode *link_node;
    
    // 在 dir_node 目录下创建一个名为 name 的文件，类型为 SFS_TYPE_LINK
    // sfs_create_inode 会负责分配磁盘 inode、分配内存 inode、并添加到目录项中
    // 成功后，link_node 的引用计数通常为 1（表示当前被打开/持有）
    int ret = sfs_create_inode(dir_node, name, SFS_TYPE_LINK, &link_node);
    if (ret != 0) {
        return ret; // 创建失败（如磁盘满、重名等）
    }
    
    // 2. 获取 SFS 内部的 inode 结构体
    // VFS 层使用的是 struct inode，这里我们需要操作 SFS 文件系统特有的 struct sfs_inode
    struct sfs_inode *sin = vop_info(link_node, sfs_inode);
    
    // 3. 将目标路径写入 inode
    // 这是软链接的核心：它的“内容”就是目标文件的路径字符串
    // 这里直接写到了 inode 的 din (Disk Inode) 结构体的一个保留字段中
    // 注意：这通常称为 "Fast Symlink"（快速软链），因为不需要分配额外的数据块来存路径
    strncpy(sin->din->symlink_target, target, SFS_MAX_SYMLINK_LEN);
    
    // 设置文件大小为路径字符串的长度
    // 例如链接到 "/bin/ls"，大小就是 7 字节
    sin->din->size = strlen(target);
    
    // 标记为“脏”，确保上述修改（路径和大小）会被回写到磁盘
    sin->dirty = true;
    
    // 4. 释放 inode 引用
    // sfs_create_inode 返回的 link_node 是处于“打开”状态的
    // 我们只是创建它，现在并没有进程要打开它读写，所以操作完成后需要减少引用计数
    // 如果引用计数降为 0，内存中的 inode 可能会被回收（但磁盘上的文件依然存在）
    vop_ref_dec(link_node);
    
    return 0;
}
```

### 5.3 路径解析处理
```c
// 查找并解析路径，支持符号链接的递归解析
// node: 起始查找目录
// path: 要查找的路径字符串
// node_store: 用于返回最终找到的 inode
// max_depth: 最大递归深度，防止符号链接死循环
int sfs_lookup_follow_links(struct inode *node, char *path, 
                           struct inode **node_store, int max_depth) {
    // 1. 防止死循环
    // 如果符号链接嵌套太深（如 A->B->A），超过限制则报错
    if (max_depth <= 0) {
        return -ELOOP;  // Too many symbolic links encountered
    }
    
    struct inode *current = node;
    // 增加起始目录的引用计数，因为我们在循环结束时会统一做 dec 操作
    vop_ref_inc(current);
    
    // 2. 路径分割循环
    // 使用 strtok 将路径按 '/' 分割，例如 "home/user/file" -> "home", "user", "file"
    // 注意：strtok 会修改 path 字符串的内容
    char *component = strtok(path, "/");
    while (component != NULL) {
        struct inode *next;
        
        // 在当前目录 current 下查找名为 component 的文件/目录
        // 如果找到，next 的引用计数会被加 1
        int ret = vop_lookup(current, component, &next);
        if (ret != 0) {
            vop_ref_dec(current); // 查找失败，释放当前目录
            return ret;
        }
        
        // 3. 检查是否为符号链接
        uint32_t type;
        vop_gettype(next, &type);
        if (type == SFS_TYPE_LINK) {
            // 读取符号链接的内容（即目标路径）
            char target[SFS_MAX_SYMLINK_LEN];
            ret = sfs_readlink(next, target, sizeof(target));
            if (ret < 0) {
                vop_ref_dec(current);
                vop_ref_dec(next); // 读取失败，释放刚才找到的 symlink inode
                return ret;
            }
            
            // 我们已经读取了目标路径，不再需要这个 symlink inode 本身了
            vop_ref_dec(next);
            
            // 4. 递归解析符号链接指向的新路径
            // 注意：这里将结果直接存回 &next，替换掉了刚才的 symlink inode
            if (target[0] == '/') {
                // 情况A: 绝对路径 (如 "/bin/ls")，从根目录开始递归查找
                ret = sfs_lookup_follow_links(sfs_get_root(), target, 
                                            &next, max_depth - 1);
            } else {
                // 情况B: 相对路径 (如 "../lib")，从当前目录开始递归查找
                ret = sfs_lookup_follow_links(current, target, 
                                            &next, max_depth - 1);
            }
            
            // 递归查找失败
            if (ret != 0) {
                vop_ref_dec(current);
                return ret;
            }
            
            // 此时 next 已经指向了符号链接最终指向的真实文件/目录
        }
        
        // 5. 步进到下一级
        // 释放当前目录的引用（因为我们已经进入了 next）
        vop_ref_dec(current);
        
        // 将当前目录更新为找到的 next
        current = next;
        
        // 继续解析路径的下一部分
        component = strtok(NULL, "/");
    }
    
    // 6. 返回结果
    // 循环结束，path 解析完毕，current 即为最终找到的 inode
    *node_store = current;
    return 0;
}
```

### 5.4 文件删除处理
```c
// 解除硬链接（通常用于删除文件）
int sfs_unlink(struct inode *dir_node, const char *name) {
    struct inode *node;
    
    // 1. 查找目标文件
    // 根据文件名在目录 dir_node 中找到对应的 inode
    // 注意：vop_lookup 成功后，node 的内存引用计数(ref_count)会 +1
    int ret = vop_lookup(dir_node, (char *)name, &node);
    if (ret != 0) {
        return ret; // 文件不存在
    }
    
    // 2. 从父目录的数据块中删除目录项
    // 这一步只是删除了“文件名”，此时文件数据仍然存在于磁盘上
    ret = sfs_remove_direntry(dir_node, name);
    if (ret != 0) {
        // 如果删除目录项失败（例如没有写权限），需要释放刚才 lookup 获取的引用
        vop_ref_dec(node);
        return ret;
    }
    
    // 3. 减少硬链接计数 (Core Logic)
    // 获取 SFS 具体实现的 inode 结构
    struct sfs_inode *sin = vop_info(node, sfs_inode);
    
    // 原子地将磁盘 inode 中的硬链接数 -1
    // 这是一个关键点：可能还有其他目录项（硬链接）指向这个文件
    int nlinks = sfs_nlinks_dec_atomic(sin);
    
    // 4. 判断是否需要真正的“物理删除”
    // 如果 nlinks 变为 0，说明没有任何文件名指向该文件了
    if (nlinks == 0) {
        // 回收 inode 及其占用的所有数据块 (Data Blocks)
        // 将 inode 位图和块位图对应的位清零
        // 注意：在标准 Unix 中，这里通常还需要检查是否有进程打开了该文件(open_count)
        // 只有 nlinks==0 且 open_count==0 时才会真正释放磁盘空间
        sfs_free_inode(node);
    }
    
    // 5. 释放内存引用
    // 对应开头 vop_lookup 增加的引用计数
    // 如果上面执行了 sfs_free_inode，这里的 dec 可能会触发内存中 inode 结构的最终销毁
    vop_ref_dec(node);
    
    return 0;

}
```

## 6. 错误处理和边界情况

### 6.1 符号链接循环检测
- 限制符号链接解析深度(如40层)
- 检测路径解析过程中的循环引用

### 6.2 权限检查
- 创建链接需要对目标目录有写权限
- 符号链接的权限检查在目标文件上进行

### 6.3 跨文件系统链接
- 硬链接不能跨文件系统

- 符号链接可以指向任意路径
